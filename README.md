# ABOUT THE PROJECT

## Projekt goal / Motivation
I am affiliated with a school that educates children with special needs. Each semester, our teachers are tasked with preparing comprehensive reports for each student, assessing them across various criteria adapted from the ICF Catalog (International Classification of Functioning, Disability and Health). These criteria are modified to suit our context, as we primarily cater to children diagnosed with ADHD or autism, rather than those with physical or mental disabilities.

The report-writing process is notably labor-intensive. It requires professionalism, adherence to specific criteria, and personalization for each student. Additionally, coordinating meetings among all teachers involved with a particular student to discuss these assessments can be challenging due to conflicting schedules.

To streamline this process, we aim to develop a web application that guides teachers through the assessment of each student. This tool will allow teachers to individually evaluate students and enable the lead teacher to consolidate these evaluations into a single comprehensive report.

Furthermore, for the final remarks section of the report, the application will provide functionality to send notes from all teachers to OpenAI. This will generate a preliminary draft by synthesizing all inputs into a unified text, which can then be refined by the lead teacher through a prompt-based correction tool or by rewriting as needed.

Ultimately, the completed assessments and notes will be processed through OpenAI to produce a final document that meets the school’s standards. This approach aims to reduce the administrative burden on our teachers and enhance the quality and efficiency of student assessments.

Of course, all names will be anonymized before any notes or assessments are submitted to OpenAI.


## Data Collection or Generation
To protect the privacy of our students, I have opted not to use actual personal data. Instead, I have created dummy data, which is stored in an Excel file. This format was chosen because our original student data is also maintained in Excel, ensuring compatibility and ease of use.

Using a Jupyter Notebook, the dummy data is read from the Excel file, modified as needed, and then stored across three separate collections in MongoDB. This setup mimics the structure and processing that would be involved with real data.

For demonstration purposes, the dataset includes examples of assessments and final assessments. Typically, these would be generated by the teachers. However, incorporating predefined examples facilitates the evaluation of the web application's proof of concept by external reviewers or during initial testing phases.


## Modelling
I use prompt engineering with a few-shot approach, allowing OpenAI to understand the expected format of the text. Note recommendations are generated using a single prompt paired with an example. The final text is produced using a single prompt alongside three examples. Additionally, follow-up prompts utilize the history from the initial final-text prompt, enabling OpenAI to recognize prior content and make appropriate corrections.


## Interpretation and Validation
Whenever a teacher opts for a text other than the one recommended by OpenAI, the cosine similarity between the two texts is calculated. A high similarity score indicates that OpenAI's output closely meets the report requirements set by the teachers. Conversely, a low score suggests inadequacies in the prompts or few-shot examples used, signaling a need for adjustment. Both the initial and manually chosen texts are stored in the database, along with the similarity scores and timestamps. This data is archived for future analysis to enhance the web application's functionality.

To evaluate the effectiveness of the prompt engineering, I used historical texts and assessments, sending them anonymously to OpenAI for text generation. These generated texts were then compared with the original texts focused on specific criteria. Since only a subset of the criteria was relevant for this webapp, it was necessary to extract just the matching sections.



# RUN THE PROGRAM

## 1. Create Python-Environment
- CTRL+Shift+p -> Create Environment
- Choose .venv
- Choose Python 3.12.1 (or similar)
- Install Libraries: pip install -r requirements.txt


## 2. Get Access to DB nd API
- Create a File with the name .env on the toplevel of the project (same folder as requirements.txt) with the following:
    - MONGODB_URI=<your own mongodb-uri> <-- if you don't have one, you can create one for free
    - OPENAI_APIKEY=<openai-api-key> <-- if you don't have one, contact me


## 3. reate Dummy-Data
- Kernel: Python 3.11 or higher
- Run Jupyter Notebook "Dummy-Data-Input"
- Check your MongoDB, if you can find 3 collections and data in each collection


## 4. Run Backend-Server
- Go to directory: cd backend
- Start backend-server: litestar run
- Start backend-server in debug-mode: litestar run -rd


## 5. Run Frontend
- Open a second terminal
- Go to directory: cd frontend
- Install dependencies: npm install
- Start frontend: npm run dev


## 6. Happy-Path
- Open a browser and type: http://localhost:8081/ or http://127.0.0.1/8081 (it should be the port that the frontend provides you)
- Go to "Beurteilen"
- Choose Student "Natasha Romanoff"
- Choose Semester "HS23"
- Write an Assessment, notes are not required but nice to have.
- Click "Bestätigen"
- Click "OK" -> you will be directed to the page "Zusammenführen" for Natasha and HS23
- Click "OK"
- Choose the "Bewertung" you find most suitable
- Change at least one note
- Click "Abschliessen"
    - -> You can see an Overview of the final assessment
    - -> You can see the similarity of your modified note and the recommended note from OpenAI
- Click "Bestätigen" -> you will be directed to the page "Finaler Text" for Natasha for HS23
- Click "Text generieren"
- Click "Korrigieren"
- Write a propmpt (e.g. Bitte füge bei den Sprachen noch Französisch hinzu)
- Click "Send"
- Click "Eigener Text"
- You can write your own text (e.g. Test Text)
- Click "Send"
    - -> You will see the similarity between your text and the first text from OpenAI
    - -> Your text will be stored in the Database
    - -> The similarity will be stored in a new Collection in MongoDB
- Click "OK"


## 7. Experiment
- You can try out different Use-Cases


# Front-End-Explanation

## Home
Here you find the project-goal and motivation behind the project

## Beurteilen
Here you can choose a student and a semester.
You create an assessment for the student by different criteria.
If you want, you can write down some notes as well.
Per student and semester you can create several assessments.
This is important, so many different teachers can assess the student individually.
All the assessment are stored in a seperate collection in the Database.

## Zusammenführen
Here you can choose a student and a semester.
You can see all the assessment that have been done for a given student and semester.
You can decide how you would like to create the final assessment.
The notes of the different teachers are combined to a note-suggestion, but you are free to create another note-entry.
If you write your own note the Cosine-Similarity will be calculated between the openAI-recommended-note and your own one.
The final-assessment will be stored with the rest of the semester-data to the Database

## Finaler Text
Here you can choose a student and all the semesters with a final assessment.
You can see an overview of the final assessment.
You have the possibility to correct something, if you are not yet happy.
If the final-assessment is ok, you can create a text.
The text is created with the openai-API.
I used prompt engineering with a few shot dataset.
All entries are anonymized, so there will be no name sent to openai.
The text will be displayed in the frontend with the name.
If you are not yet happy with the text, you can write your own prompt to give openai specific information, how you would like to change the text. (TODO)
You can also write your own text.
If you write your own text the Cosine-Similarity will be calculated between the openAI-recommended-text and your own one.
If you are happy with the text, it will be stored in the Database with the rest of the semester-data.

## Additional Pages
- Student: If you'd like to create your own student, you can use '/base-data'-URL.
- Semester: If you'd like to create your own semester, you can use '/semester-data'-URL.



# Other requirements (that everyone should already have from other moduls):
- pyenv
- python 3.12
- juypter notebook
- node.js


## node.js

### Windows
Step 1:
Install Node.js and npm
Download and install Node.js:
Go to the official Node.js website.
Download the installer for your operating system (Windows in your case).
Run the installer and follow the installation instructions. Ensure you check the option to add Node.js to the system PATH during installation.
Verify the installation:
Open a new Command Prompt or PowerShell window.
Run the following commands to check if Node.js and npm are installed and accessible:
sh
Code kopieren
node -v
npm -v
You should see version numbers for both Node.js and npm.

Step 2:
Add Node.js and npm to PATH (if not already added)
Check if Node.js is added to PATH:
Open a new Command Prompt or PowerShell window.
Run the following command to check the current PATH environment variable:
sh
Code kopieren
echo %PATH%
Look for the directory where Node.js is installed, usually something like C:\Program Files\nodejs\.
Manually add Node.js to PATH (if needed):
Right-click on the Start menu and select System.
Click on Advanced system settings.
In the System Properties window, click on the Environment Variables button.
In the Environment Variables window, find and select the Path variable under System variables, then click on Edit.
Click on New and add the path to the directory where Node.js is installed, e.g., C:\Program Files\nodejs\.
Click OK to close all windows.

Step 3:
Restart your laptop to ensure it reflects
